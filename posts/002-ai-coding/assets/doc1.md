# 公司内部 Vibe Coding 教程（面向第一次上手的同事）

这份教程基于 OpenClaw 作者 Peter Steinberger（@steipete）对 *vibe coding / agentic engineering* 的实践经验整理，目标是让你在**不熟悉 vibe coding**的情况下，也能用一套稳定流程把需求快速落地，同时把质量、可控性和团队协作都顾好。

---

## 1. 什么是 Vibe Coding（我们在公司语境里的定义）

**Vibe coding**不是“让 AI 随便写代码”。更准确的说法是：

* 你负责：**目标、品味、取舍、验收标准、风险控制**
* AI 负责：**读代码、提出方案、写实现、补测试、补文档、做重构和重复劳动**
* 核心能力：把开发从“手敲代码”变成“**指挥 + 验证闭环**”

一句话：**产能来自闭环**——让 AI 能运行、能测、能验证，然后迭代。

---

## 2. 最重要的原则：闭环优先（能跑 > 好看）

Steinberger 反复强调的第一原则是“closing the loop（闭环）”：

* 任何新想法，优先做成**可执行、可验证**的形态（最推荐：CLI）
* 因为 CLI 最容易让 agent **直接运行命令 → 看输出/跑测试 → 修复 → 再验证**
* UI/复杂交互往往会拖慢反馈，破坏闭环

**团队落地建议：**

* 新功能先做一个 `cli` 或最小 API endpoint，哪怕很丑
* “跑起来 + 可测 + 可验证”之后再做 UI 和体验优化

---

## 3. 推荐的工作流：Discuss → Build → Verify → Ship

### 3.1 Discuss（先聊清楚，不要急着写代码）

你先让 AI 做“读代码 + 方案权衡 + 风险点”，明确方向后再开工。

可直接复制的提示词（中文）：

* **“先不要写代码，先讨论。请读完相关代码路径，给我 3 个实现方案：每个方案的改动点、风险、测试策略、回滚方式。”**
* **“先给我 options，再推荐一个你认为最稳的，并说明为什么。”**
* **“列出验收标准（acceptance criteria）和你准备新增/修改的测试。”**

**这一阶段你要做的事：**

* 选方案（性能/风险/交付速度/可维护性）
* 把“模糊目标”变成“可验收的标准”

---

### 3.2 Build（再开工，明确让它动手）

当你满意方案后，再明确下指令让它开始改代码：

* **“可以开始写代码了。按你推荐的方案实现。要求：保持变更小而集中；每一步都能跑测试。”**
* **“先实现最小闭环版本（MVP），只要通过验收标准即可。”**

**关键习惯：小步提交 / 小步可验证**

* 尽量让 AI 一次做一个清晰的小目标（一个函数、一个模块、一个测试集）
* 每次改完立刻跑对应测试/命令，避免堆积

---

### 3.3 Verify（验证不是可选项，是 vibe coding 的发动机）

Steinberger 的观点很直接：瓶颈从“写代码”变成“验证与思考”。

你应该让 AI 负责：

* 跑测试（单测/集成/端到端）
* 给出复现步骤
* 补测试用例（尤其是你担心的边界）

可用提示词：

* **“实现完成后，请你：1）跑 tests；2）列出你新增/修改的测试；3）给出 3 个潜在边界问题并加测试覆盖。”**
* **“请写一个失败用例来证明我们修复了 bug（回归测试）。”**

---

### 3.4 Ship（交付节奏：主干可发布，减少状态切换）

Steinberger 偏好“主干持续推进”的节奏（尤其适合个人/小团队）：

* 保持 main 分支随时可发布
* 少搞长期漂移的大分支
* 通过小步可验证的改动持续前进

**团队建议：**

* PR 尽量小（可评审）
* 每个 PR 必须带：测试/验证说明、回滚点、风险备注

---

## 4. 输入侧技巧：把“含糊”变“规格”（语音脑暴也行）

他的一个实用技巧是：

1. 先用语音/随口描述把想法倾倒出来
2. 再让 AI 把内容整理成结构化 spec（需求、边界、验收）

你可以用这份 Spec 模板（内部推荐）：

**Spec 模板（可粘贴到 issue/PR 描述）**

* 背景/问题：
* 目标（What）：
* 非目标（Not doing）：
* 用户故事/使用场景：
* 约束（性能/兼容/安全/合规）：
* 方案选项（可选）：
* 验收标准（必须可验证）：
* 测试计划：
* 回滚方案：
* 风险点 & 监控指标：

---

## 5. 工具侧原则：能用 CLI 就用 CLI（减少上下文税）

他的观点很明确：很多“插件/协议化工具”会带来**上下文税**（描述复杂、占用上下文、错误难定位）。CLI 则更直接、可组合、好验证。

**团队落地建议：**

* 优先把内部工具做成 CLI（例如：生成报表、跑数据校验、执行迁移、拉取样本）
* AI 能够调用 CLI → 你就能得到稳定闭环

额外技巧：

* 没有后台任务管理时，用 `tmux`/终端会话维持 dev server、watcher、长测试

---

## 6. 并行多 agent 的用法（但别过度“自动编排”）

一个很实用的打法：**开 3–8 个 agent 并行**，你当“调度器”。

推荐分工（例）：

* Agent A：主线功能实现
* Agent B：测试补齐 + 回归用例
* Agent C：文档/README/示例更新
* Agent D：重构/清理重复代码/类型检查/格式化

**关键点：**

* 并行不是“全自动”，你要持续做取舍、合并方向、控制范围
* 每个 agent 都给清晰边界：任务、文件范围、验收方式

---

## 7. 代码库要“为 agent 可维护”而设计（别用洁癖挡进度）

Steinberger 提到一个心态转换：

* 接受 AI 写的代码不一定像你手写那样“优雅”
* 但你可以把代码库改造成：**更好导航、更好搜索、更一致**

团队可执行的工程化动作：

* 给关键模块补 `README.md` 或顶部注释：职责、入口、调用链
* 统一项目结构/命名（别一会儿 service 一会儿 manager）
* 保持测试可跑、脚本可用、lint/format 稳定
* 增加 `AGENTS.md`（见下一节）让新 agent 快速上手

---

## 8. 建议新增一个 `AGENTS.md`（团队“长期提示词”）

在仓库根目录放一个 `AGENTS.md`，写清楚：

* 项目目标与边界
* 架构入口（关键目录说明）
* 本地运行/测试命令（最重要）
* 代码风格与约束（lint、格式化、提交规范）
* 禁止事项（例如不能改公共接口、不能引入某类依赖）
* 常见坑与排查路径（日志位置、feature flag、配置）

这会显著降低 agent 每次“重新理解项目”的成本。

---

## 9. 常见反模式（踩坑预警）

* **反模式 1：跳过 Discuss，直接让 AI 大改代码**

  * 结果：改动范围失控、回滚困难、评审痛苦
* **反模式 2：没有验收标准**

  * 结果：你在“感觉它差不多了”里无限循环
* **反模式 3：不跑测试，只看 diff**

  * 结果：闭环断裂，bug 迟早反噬
* **反模式 4：把 agent 当“万能全自动”**

  * 结果：方向漂移、架构崩坏、团队不可维护

---

## 10. 安全与合规（内部必须遵守）

* 不要把密钥、客户数据、敏感日志直接贴给模型
* 让 agent 执行命令前，明确禁止破坏性操作（如删除、生产环境改动）
* 对生产/数据迁移相关任务：

  * 必须有人类 owner review
  * 必须有回滚方案与监控点

（如果你们有公司内部 AI 使用规范，这一节应当链接到规范页面并以其为准。）

---

## 11. 一套“当天就能用”的上手练习（30–60 分钟）

**练习任务：修一个小 bug 或加一个小配置项**

流程：

1. 让 AI 先 Discuss：定位代码路径 + 方案 + 测试计划
2. 你选方案，要求它 Build：最小闭环实现
3. 让 AI Verify：补回归测试 + 跑测试 + 给复现/验证步骤
4. 形成一个小 PR：描述清晰、可回滚、可验收

你会立刻建立 vibe coding 的正确肌肉记忆：**对话驱动 + 闭环验证**。

---

## 12. 提示词小抄（直接复制）

**讨论阶段**

* “先不要写代码。读完相关代码后，给 3 个方案+风险+测试策略。”
* “请列出你需要确认的假设/不确定点，并告诉我如何验证它们。”

**实现阶段**

* “按推荐方案实现，要求改动小而集中，每一步能跑测试。”
* “先做最小闭环版本，满足验收标准即可。”

**验证阶段**

* “请补回归测试，并跑完整测试集。把运行命令和结果贴出来。”
* “列出 3 个边界条件并覆盖测试。”

**收尾阶段**

* “更新文档/示例，并写清楚如何验证这个功能。”
* “给出回滚方案与可能的监控指标。”

---

如果你告诉我你们常用的技术栈（例如：Java/Spring、Go、Python、Node、iOS/Android）和日常 CI/测试命令，我可以把这份教程进一步“落到你们仓库的真实命令与规范上”，并给一份可直接落地的 `AGENTS.md` 模板 + PR 模板。
